# ğŸš€ RAG Streamlit App con Ollama

Un'applicazione per il Recupero Aumentato con Generazione (RAG) su documenti PDF/DOCX, in locale grazie a Ollama, LangChain e ChromaDB.

## ğŸ” Modello Consigliato
L'applicazione Ã¨ configurata per funzionare ottimamente con:
deepseek-r1:7b

Questo Ã¨ il modello specifico utilizzato durante lo sviluppo. Puoi comunque sperimentare con altri modelli supportati da Ollama.

## âœ¨ FunzionalitÃ  principali

- **Caricamento temporaneo** di documenti PDF/DOCX per interrogazione immediata
- **Archiviazione persistente** dei documenti indicizzati in ChromaDB
- **Supporto multi-modello** tramite Ollama (deepseek, mistral, llama3, ecc.)
- **Interfaccia intuitiva** realizzata con Streamlit
- **Elaborazione 100% locale** senza dipendenze da servizi cloud

## ğŸ›  Stack Tecnologico

| Tecnologia | Descrizione |
|------------|-------------|
| [Ollama](https://ollama.com/) | Framework per eseguire LLM localmente |
| [LangChain](https://www.langchain.com/) | Framework per applicazioni LLM |
| [ChromaDB](https://www.trychroma.com/) | Database vettoriale open-source |
| [Streamlit](https://streamlit.io/) | Framework per creare app web in Python |

## ğŸ— Struttura del progetto

â”œâ”€â”€ app.py # Codice principale dell'applicazione

â”œâ”€â”€ requirements.txt # Dipendenze Python

â”œâ”€â”€ .gitignore # Esclude file temporanei e chroma_db/

â””â”€â”€ chroma_db/ # Directory dei documenti indicizzati (ignorata da git)


## ğŸš€ Guida all'uso

### Prerequisiti
1. Installa [Ollama](https://ollama.com/)
2. Scarica un modello LLM (es: `ollama pull deepseek`)

### Avvio dell'applicazione
```bash
pip install -r requirements.txt  # Installa le dipendenze
streamlit run app.py            # Avvia l'applicazione

Flusso di lavoro

    Carica un documento (PDF o DOCX)

    Scegli tra:

        "Chiedi!": Interroga il documento temporaneamente

        "Indicizza documento": Aggiungi il documento al database permanente

    Inserisci la tua domanda e ottieni le risposte dal modello LLM
